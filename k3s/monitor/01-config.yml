# Configurações prometheus
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus
  namespace: monitor
data:
  prometheus.yml: |-
    global:
      scrape_interval: 1m
      evaluation_interval: 1m
      scrape_timeout: 15s

    rule_files:
      - 'alerts.yml'

    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager.apcode.com.br

    scrape_configs:
    - job_name: 'Servidor cluster.apcode.com.br'
      static_configs:
        - targets: [ 'cluster.apcode.com.br:9100' ]
          labels: { 'env': 'servidores' }  
    - job_name: 'Interface de Login'
      metrics_path: '/actuator/prometheus'
      static_configs:
        - targets: [ 'login.apcode.com.br' ]
          labels: { 'env': 'sistemas' }  
    - job_name: 'Portal de Sistemas'
      metrics_path: '/actuator/prometheus'
      static_configs:
        - targets: [ 'portal.apcode.com.br' ]
          labels: { 'env': 'sistemas' }  
    - job_name: 'Módulo de Controle'
      metrics_path: '/actuator/prometheus'
      static_configs:
        - targets: [ 'controle.apcode.com.br' ]
          labels: { 'env': 'sistemas' }  
  alerts.yml: |-
    groups:
      - name: Alertas de Servidores
        rules:
      
        # Alerta de serviço indisponível por mais de 2 minutos
        - alert: Servico_Indisponivel
          expr: up == 0
          for: 2m
          labels:
            severity: URGENTE
          annotations:
            title: '*{{ $labels.job }}* esta offline'
            summary: '*{{ $labels.job }}* voltou a estar online'
      
        # Alerta para uso de mais de 90% de CPU
        - alert: Uso_Excessivo_CPU
          expr: 100 - (avg by(job) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
          for: 5m
          labels:
            severity: Atenção
          annotations:
            title: 'Uso constante de mais de 90% de CPU do *{{ $labels.job }}*'
            summary: 'Uso de CPU do *{{ $labels.job }}* normalizado para menos de 90%'

        # Alerta para menos de 100MB de memória
        - alert: Pouca_Memoria
          expr: node_memory_MemAvailable_bytes < 100000000
          for: 5m
          labels:
            severity: Atenção
          annotations:
            title: 'Menos de 100MB de memória disponível para *{{ $labels.job }}*'
            summary: 'Memória disponível para *{{ $labels.job }}* aumentou para mais de 100MB'

        # Alerta para load average acima de 5
        - alert: Baixa_Performance
          expr: node_load5 > 8
          for: 5m
          labels:
            severity: Atenção
          annotations:
            title: 'Baixa performance do *{{ $labels.job }}*. Usuários podem estar sendo afetados'
            summary: 'Performance restaurada do *{{ $labels.job }}*'

        # Disco sem espaço
        #- alert: Disco_Sem_Espaco
        #  expr: (node_filesystem_avail_bytes{mountpoint='/'}  * 100) / node_filesystem_size_bytes{mountpoint='/'} < 10
        #  for: 5m
        #  labels:
        #    severity: URGENTE
        #  annotations:
        #    title: 'Disco de *{{ $labels.job }}* com menos de 10% de espaço'
        #    summary: 'Disco de *{{ $labels.job }}* aumentou para mais de 10% de espaço'

        # Disco extremamente sem espaço
        - alert: Disco_Sem_Espaco
          expr: (node_filesystem_avail_bytes{mountpoint='/'}  * 100) / node_filesystem_size_bytes{mountpoint='/'} < 2
          for: 5m
          labels:
            severity: URGENTE
          annotations:
            title: 'RISCO DE TRAVAMENTO DO SERVIDOR *{{ $labels.job }}*. Menos de 2% de espaço'
            summary: 'Disco de *{{ $labels.job }}* aumentou para mais de 2% de espaço'

# Configurações grafana tempo
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: tempo
  namespace: monitor
data:
  tempo-config.yml: |-
    server:
      http_listen_port: 3200

    distributor:
      receivers:
        zipkin:

    storage:
      trace:
        backend: local
        wal:
          path: /tmp/tempo/wal
        local:
          path: /tmp/tempo/blocks

# Configurações grafana loki
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki
  namespace: monitor
data:
  loki-config.yml: |-
    auth_enabled: false

    server:
      http_listen_port: 3100
      grpc_listen_port: 9096

    ingester:
      wal:
        enabled: true
        dir: /tmp/wal
      lifecycler:
        address: 127.0.0.1
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
        final_sleep: 0s
      chunk_idle_period: 1h       # Any chunk not receiving new logs in this time will be flushed
      max_chunk_age: 1h           # All chunks will be flushed when they hit this age, default is 1h
      chunk_target_size: 1048576  # Loki will attempt to build chunks up to 1.5MB, flushing first if chunk_idle_period or max_chunk_age is reached first
      chunk_retain_period: 30s    # Must be greater than index read cache TTL if using an index cache (Default index read cache TTL is 5m)
      max_transfer_retries: 0     # Chunk transfers disabled

    schema_config:
      configs:
        - from: 2021-06-30
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h

    storage_config:
      boltdb_shipper:
        active_index_directory: /tmp/loki/boltdb-shipper-active
        cache_location: /tmp/loki/boltdb-shipper-cache
        cache_ttl: 24h         # Can be increased for faster performance over longer query periods, uses more disk space
        shared_store: filesystem
      filesystem:
        directory: /tmp/loki/chunks

    compactor:
      working_directory: /tmp/loki/boltdb-shipper-compactor
      shared_store: filesystem

    limits_config:
      reject_old_samples: true
      reject_old_samples_max_age: 168h

    chunk_store_config:
      max_look_back_period: 0s

    table_manager:
      retention_deletes_enabled: false
      retention_period: 0s

    ruler:
      storage:
        type: local
        local:
          directory: /tmp/loki/rules
      rule_path: /tmp/loki/rules-temp
      alertmanager_url: http://alertmanager:9093
      ring:
        kvstore:
          store: inmemory
      enable_api: true
